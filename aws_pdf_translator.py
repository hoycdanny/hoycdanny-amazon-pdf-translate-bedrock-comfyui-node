# -*- coding: utf-8 -*-
"""
AWS PDFç¿»è­¯å™¨ - ComfyUIç¯€é»
ä½¿ç”¨Amazon Bedrock AIå’ŒAmazon Translateé€²è¡ŒPDFç¿»è­¯
"""

import os
import logging
import json
from typing import List, Tuple, Any
import torch
import numpy as np
from PIL import Image, ImageDraw, ImageFont

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AWSPDFTranslator:
    """AWS PDFç¿»è­¯å™¨ç¯€é»"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "pdf_source_path": ("STRING", {
                    "default": "/path/to/source.pdf",
                    "multiline": False,
                    "placeholder": "PDFä¾†æºæ–‡ä»¶è·¯å¾‘"
                }),
                "pdf_target_path": ("STRING", {
                    "default": "/path/to/translated_output.txt",
                    "multiline": False,
                    "placeholder": "ç¿»è­¯è¼¸å‡ºæ–‡ä»¶è·¯å¾‘"
                }),
                "source_language": ("STRING", {
                    "default": "en",
                    "multiline": False,
                    "placeholder": "æºèªè¨€ä»£ç¢¼ (å¦‚: en, zh, ja)"
                }),
                "target_language": ("STRING", {
                    "default": "zh-TW",
                    "multiline": False,
                    "placeholder": "ç›®æ¨™èªè¨€ä»£ç¢¼ (å¦‚: zh-TW, ja, ko)"
                }),
                "aws_region": ("STRING", {
                    "default": "us-east-1",
                    "multiline": False,
                    "placeholder": "AWSå€åŸŸ"
                }),
                "excluded_words": ("STRING", {
                    "default": "<æ’é™¤ç¿»è­¯çš„å­—è©>\næ¯å€‹æ›è¡Œä»£è¡¨ä¸€å€‹å–®å­—\nä¾‹å¦‚ï¼š\nAWS\nAmazon\nElastiCache\nRedis\nRedis OSS\nMemoryDB\nValkey\nIDC",
                    "multiline": True,
                    "placeholder": "æ¯è¡Œè¼¸å…¥ä¸€å€‹ä¸éœ€è¦ç¿»è­¯çš„è©å½™"
                })
            }
        }
    
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("status_image", "translation_report")
    FUNCTION = "translate_pdf"
    CATEGORY = "AWS/Translation"
    
    def translate_pdf(self, pdf_source_path: str, pdf_target_path: str, 
                     source_language: str, target_language: str, 
                     aws_region: str, excluded_words: str) -> Tuple[torch.Tensor, str]:
        """ä¸»è¦ç¿»è­¯å‡½æ•¸"""
        try:
            logger.info("ğŸš€ AWS PDF Translator v4.2 - Stable & Compatible")
            logger.info(f"ğŸ“„ Source: {pdf_source_path}")
            logger.info(f"ğŸ“„ Target: {pdf_target_path}")
            logger.info(f"ğŸŒ Translation: {source_language} â†’ {target_language}")
            
            # è™•ç†æ’é™¤è©å½™ - æ”¯æŒæ›è¡Œå’Œé€—è™Ÿåˆ†éš”
            excluded_list = []
            if excluded_words.strip():
                # é¦–å…ˆæŒ‰æ›è¡Œåˆ†å‰²
                lines = excluded_words.strip().split('\n')
                for line in lines:
                    line = line.strip()
                    # éæ¿¾æ‰æŒ‡å°æ€§æ–‡å­—
                    if line and not line.startswith('<') and not line.startswith('æ¯å€‹æ›è¡Œ') and not line.startswith('ä¾‹å¦‚'):
                        # å¦‚æœè¡Œä¸­åŒ…å«é€—è™Ÿï¼Œå†æŒ‰é€—è™Ÿåˆ†å‰²
                        if ',' in line:
                            words = [word.strip() for word in line.split(',') if word.strip()]
                            excluded_list.extend(words)
                        else:
                            excluded_list.append(line)
            
            # å»é‡ä¸¦ä¿æŒé †åº
            excluded_list = list(dict.fromkeys(excluded_list))
            if excluded_list:
                logger.info(f"ğŸš« Excluded words ({len(excluded_list)}): {excluded_list}")
            else:
                logger.info("ğŸš« No excluded words specified")
            
            # æ­¥é©Ÿ1: æå–PDFæ–‡å­—
            logger.info("ğŸ“– Extracting text from PDF with AI content analysis")
            pages_text = self._extract_pdf_text(pdf_source_path, aws_region)
            
            if not pages_text:
                return self._create_error_result("No text extracted from PDF")
            
            # æ­¥é©Ÿ2: ç¿»è­¯æ–‡å­—
            logger.info(f"ğŸŒ Translating {len(pages_text)} pages with Amazon Translate")
            translated_pages = self._translate_pages(pages_text, source_language, target_language, aws_region, excluded_list)
            
            if not translated_pages:
                return self._create_error_result("Translation failed")
            
            # æ­¥é©Ÿ3: å‰µå»ºç¿»è­¯æ–‡å­—æ–‡ä»¶
            logger.info("ğŸ“ Creating translation text file")
            success = self._create_translation_text_file(pages_text, translated_pages, pdf_target_path)
            
            if not success:
                return self._create_error_result("Failed to create translation file")
            
            # ç”Ÿæˆç‹€æ…‹å ±å‘Š
            txt_output_path = pdf_target_path.replace('.pdf', '_translation.txt')
            status_report = self._generate_status_report(len(pages_text), txt_output_path, pages_text, translated_pages)
            
            # å‰µå»ºæˆåŠŸç‹€æ…‹åœ–åƒ
            status_image = self._create_success_image()
            
            logger.info("âœ… Translation completed successfully!")
            return (status_image, status_report)
            
        except Exception as e:
            logger.error(f"âŒ Translation failed: {e}")
            return self._create_error_result(f"Translation failed: {str(e)}")
    
    def _extract_pdf_text(self, pdf_path: str, aws_region: str = None) -> List[str]:
        """æå–PDFæ–‡å­—ï¼ˆåŒ…å«åœ–ç‰‡OCRï¼‰"""
        try:
            import pdfplumber
            import fitz  # PyMuPDF
            
            pages_text = []
            
            # ä½¿ç”¨ pdfplumber æå–æ–‡å­—
            with pdfplumber.open(pdf_path) as pdf:
                # åŒæ™‚ä½¿ç”¨ PyMuPDF è™•ç†åœ–ç‰‡
                pdf_doc = fitz.open(pdf_path)
                
                for i, page in enumerate(pdf.pages):
                    logger.info(f"  ğŸ“„ Processing page {i+1}...")
                    
                    # æ–¹æ³•1: æå–ç´”æ–‡å­—
                    text = page.extract_text()
                    
                    # èª¿è©¦ä¿¡æ¯
                    logger.info(f"  ğŸ“Š Page {i+1} text analysis:")
                    logger.info(f"      Text length: {len(text.strip()) if text else 0} chars")
                    logger.info(f"      Word count: {len(text.strip().split()) if text else 0} words")
                    logger.info(f"      Line count: {len([line for line in text.split('\\n') if line.strip()]) if text else 0} lines")
                    logger.info(f"      Text preview: '{(text.strip()[:100] + '...') if text and len(text.strip()) > 100 else (text.strip() if text else 'No text')}'")
                    
                    # æ–¹æ³•2: æ™ºèƒ½æª¢æ¸¬æ˜¯å¦éœ€è¦OCR (åŸºæ–¼åœ–ç‰‡æ•¸é‡)
                    needs_ocr = False
                    ocr_reason = ""
                    
                    # æª¢æŸ¥é é¢æ˜¯å¦åŒ…å«åœ–ç‰‡
                    image_list = pdf_doc[i].get_images()
                    has_images = len(image_list) > 0
                    
                    if not text or len(text.strip()) < 50:  # æ–‡å­—å¾ˆå°‘
                        needs_ocr = True
                        ocr_reason = "text too short (<50 chars)"
                    elif has_images and len(text.strip()) < 300:  # æœ‰åœ–ç‰‡ä¸”æ–‡å­—ä¸å¤š
                        needs_ocr = True
                        ocr_reason = f"has {len(image_list)} images with limited text (<300 chars)"
                    elif text and len(text.strip().split()) < 15:  # è©æ•¸å¾ˆå°‘
                        needs_ocr = True
                        ocr_reason = "very few words (<15 words)"
                    
                    logger.info(f"      Images on page: {len(image_list)}")
                    logger.info(f"      OCR needed: {needs_ocr} ({ocr_reason if needs_ocr else 'sufficient text content'})")
                    
                    if needs_ocr:
                        logger.info(f"  ğŸ–¼ï¸ Page {i+1} appears to be image-heavy, trying OCR...")
                        ocr_text = self._extract_text_from_images(pdf_doc[i], aws_region)
                        if ocr_text and len(ocr_text.strip()) > len(text.strip() if text else ""):
                            # å¦‚æœOCRæå–çš„å…§å®¹æ›´å¤šï¼Œä½¿ç”¨OCRçµæœ
                            text = text + "\n\n" + ocr_text if text else ocr_text
                            logger.info(f"  âœ… OCR enhanced content: {len(ocr_text)} additional characters")
                        elif ocr_text:
                            logger.info(f"  â„¹ï¸ OCR found {len(ocr_text)} chars, keeping both text and OCR content")
                            text = text + "\n\n" + ocr_text if text else ocr_text
                        else:
                            logger.warning(f"  âš ï¸ OCR failed to extract any text from page {i+1}")
                    else:
                        logger.info(f"  ğŸ“ Page {i+1} has sufficient text, skipping OCR")
                    
                    if text:
                        logger.info(f"  ğŸ¤– AI analyzing page {i+1} content...")
                        # ä½¿ç”¨AIæ¸…ç†å’Œéæ¿¾æ–‡å­—
                        cleaned_text = self._ai_filter_content(text, aws_region)
                        if cleaned_text:
                            pages_text.append(cleaned_text)
                    else:
                        logger.warning(f"  âš ï¸ No text found on page {i+1}")
                
                pdf_doc.close()
            
            logger.info(f"âœ… AI extracted and filtered text from {len(pages_text)} pages")
            return pages_text
            
        except Exception as e:
            logger.error(f"âŒ Failed to extract PDF text: {e}")
            return []
    
    def _extract_text_from_images(self, page, aws_region: str) -> str:
        """å¾é é¢åœ–ç‰‡ä¸­æå–æ–‡å­—ï¼ˆä½¿ç”¨AWS Textractæˆ–æœ¬åœ°OCRï¼‰"""
        try:
            # æ–¹æ³•1: å˜—è©¦ä½¿ç”¨ AWS Textract (æ›´æº–ç¢º)
            if aws_region:
                try:
                    return self._aws_textract_ocr(page, aws_region)
                except Exception as e:
                    logger.warning(f"AWS Textract failed: {e}, falling back to local OCR")
            
            # æ–¹æ³•2: ä½¿ç”¨æœ¬åœ° OCR (Tesseract)
            return self._local_tesseract_ocr(page)
            
        except Exception as e:
            logger.error(f"âŒ OCR failed: {e}")
            return ""
    
    def _aws_textract_ocr(self, page, aws_region: str) -> str:
        """ä½¿ç”¨ AWS Textract é€²è¡Œ OCR"""
        try:
            import boto3
            import io
            import fitz  # æ·»åŠ é€™å€‹å°å…¥
            
            # å°‡é é¢è½‰æ›ç‚ºåœ–ç‰‡
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2xæ”¾å¤§æé«˜OCRæº–ç¢ºåº¦
            img_data = pix.tobytes("png")
            
            # èª¿ç”¨ AWS Textract
            textract_client = boto3.client('textract', region_name=aws_region)
            
            response = textract_client.detect_document_text(
                Document={'Bytes': img_data}
            )
            
            # æå–æ–‡å­—
            extracted_text = []
            for block in response['Blocks']:
                if block['BlockType'] == 'LINE':
                    extracted_text.append(block['Text'])
            
            result = '\n'.join(extracted_text)
            logger.info(f"  ğŸ” AWS Textract extracted {len(result)} characters")
            return result
            
        except Exception as e:
            logger.error(f"AWS Textract OCR failed: {e}")
            raise e
    
    def _local_tesseract_ocr(self, page) -> str:
        """ä½¿ç”¨æœ¬åœ° Tesseract é€²è¡Œ OCR"""
        try:
            import pytesseract
            from PIL import Image
            import io
            import fitz  # æ·»åŠ é€™å€‹å°å…¥
            
            # å°‡é é¢è½‰æ›ç‚ºåœ–ç‰‡
            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2xæ”¾å¤§æé«˜OCRæº–ç¢ºåº¦
            img_data = pix.tobytes("png")
            
            # è½‰æ›ç‚ºPILåœ–ç‰‡
            img = Image.open(io.BytesIO(img_data))
            
            # ä½¿ç”¨ Tesseract OCR (æ”¯æŒä¸­è‹±æ–‡)
            custom_config = r'--oem 3 --psm 6 -l eng+chi_tra+chi_sim'
            text = pytesseract.image_to_string(img, config=custom_config)
            
            logger.info(f"  ğŸ” Tesseract OCR extracted {len(text)} characters")
            return text.strip()
            
        except ImportError:
            logger.warning("âš ï¸ pytesseract not installed, skipping local OCR")
            return ""
        except Exception as e:
            logger.error(f"Local Tesseract OCR failed: {e}")
            return ""
    
    def _ai_filter_content(self, text: str, aws_region: str) -> str:
        """ä½¿ç”¨AIæ™ºèƒ½éæ¿¾å…§å®¹"""
        if not text or len(text.strip()) < 10:
            return text
        
        try:
            import boto3
            import json
            
            bedrock_client = boto3.client('bedrock-runtime', region_name=aws_region)
            
            # æ§‹å»ºAIåˆ†æprompt
            prompt = f"""è«‹åˆ†æä»¥ä¸‹å¾PDFæå–çš„æ–‡å­—ï¼Œä¿ç•™ç°¡å ±çš„æ ¸å¿ƒå…§å®¹ï¼Œç§»é™¤ä¸å¿…è¦çš„å…ƒæ•¸æ“šã€‚

ä¿ç•™ä»¥ä¸‹å…§å®¹ï¼š
- æ¨™é¡Œå’Œä¸»è¦å…§å®¹
- æŠ€è¡“èªªæ˜å’ŒåŠŸèƒ½æè¿°
- é‡è¦çš„æ¥­å‹™ä¿¡æ¯
- ç”¢å“ç‰¹æ€§å’Œå„ªå‹¢

ç§»é™¤ä»¥ä¸‹å…§å®¹ï¼š
- ç‰ˆæ¬Šè²æ˜ (Â© 2025, Amazon Web Services, Inc...)
- é ç¢¼å’Œé é¢æ¨™è¨˜
- "All rights reserved" ç­‰æ³•å¾‹è²æ˜
- é‡è¤‡çš„å…¬å¸å…è²¬è²æ˜

é‡è¦ï¼šä¿æŒå…§å®¹çš„å®Œæ•´æ€§å’Œå¯è®€æ€§ï¼Œä¸è¦éåº¦åˆªæ¸›ã€‚

åŸå§‹æ–‡å­—ï¼š
{text}

æ¸…ç†å¾Œçš„å…§å®¹ï¼š"""

            # èª¿ç”¨Claudeé€²è¡Œå…§å®¹åˆ†æ
            body = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 1000,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            }
            
            response = bedrock_client.invoke_model(
                modelId="anthropic.claude-3-sonnet-20240229-v1:0",
                body=json.dumps(body)
            )
            
            response_body = json.loads(response['body'].read())
            filtered_content = response_body['content'][0]['text'].strip()
            
            # é©—è­‰AIéæ¿¾çµæœ
            if len(filtered_content) > 10 and len(filtered_content) < len(text) * 1.2:
                logger.info(f"ğŸ¤– AI filtered content: {len(text)} â†’ {len(filtered_content)} chars")
                return filtered_content
            else:
                logger.warning("ğŸ¤– AI filtering result seems invalid, using fallback")
                return self._fallback_filter_content(text)
                
        except Exception as e:
            logger.warning(f"ğŸ¤– AI filtering failed: {e}, using fallback")
            return self._fallback_filter_content(text)
    
    def _fallback_filter_content(self, text: str) -> str:
        """å›é€€çš„å…§å®¹éæ¿¾æ–¹æ³•"""
        import re
        
        # ç°¡å–®çš„æ­£å‰‡è¡¨é”å¼éæ¿¾
        copyright_patterns = [
            r'Â©\s*\d{4}.*?All rights reserved\.?',
            r'Copyright.*?\d{4}.*?reserved\.?',
            r'Â© \d{4}, Amazon Web Services.*?reserved\.?',
            r'Amazon Web Services, Inc\. or its affiliates\. All rights reserved\.?'
        ]
        
        cleaned_text = text
        for pattern in copyright_patterns:
            cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE | re.DOTALL)
        
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        
        return cleaned_text
    
    def _translate_pages(self, pages_text: List[str], source_lang: str, target_lang: str, 
                        aws_region: str, excluded_words: List[str]) -> List[str]:
        """ç¿»è­¯æ‰€æœ‰é é¢"""
        try:
            import boto3
            
            translate_client = boto3.client('translate', region_name=aws_region)
            translated_pages = []
            
            for i, text in enumerate(pages_text):
                logger.info(f"  ğŸ”„ Translating page {i+1}")
                
                # ç¿»è­¯æ–‡å­—ï¼ˆä¿è­·æ’é™¤è©å½™ï¼‰
                translated_text = self._translate_with_protection(text, source_lang, target_lang, translate_client, excluded_words)
                translated_pages.append(translated_text)
                
                logger.info(f"    âœ… Page {i+1} translated")
            
            return translated_pages
            
        except Exception as e:
            logger.error(f"âŒ Translation failed: {e}")
            return []
    
    def _translate_with_protection(self, text: str, source_lang: str, target_lang: str, 
                                  translate_client, excluded_words: List[str]) -> str:
        """ç¿»è­¯æ–‡å­—ä¸¦ä¿è­·æ’é™¤è©å½™"""
        logger.info(f"ğŸ” DEBUG: Processing text: '{text[:100]}...'")
        logger.info(f"ğŸ” DEBUG: Excluded words list: {excluded_words}")
        
        if not excluded_words:
            logger.info("ğŸ” DEBUG: No excluded words, proceeding with normal translation")
            return self._translate_text(text, source_lang, target_lang, translate_client)
        
        # æ­¥é©Ÿ1: ç”¨æ•¸å­—æ¨™è¨˜ä¿è­·æ’é™¤è©å½™
        protected_text = text
        word_map = {}
        
        # æŒ‰é•·åº¦æ’åºï¼Œå…ˆè™•ç†é•·è©å½™ï¼ˆé¿å…çŸ­è©å½™è¦†è“‹é•·è©å½™ï¼‰
        sorted_words = sorted([w.strip() for w in excluded_words if w.strip()], key=len, reverse=True)
        logger.info(f"ğŸ” DEBUG: Sorted words for protection: {sorted_words}")
        
        for i, word in enumerate(sorted_words):
            if word in text:
                # ä½¿ç”¨ç´”æ•¸å­—æ¨™è¨˜ï¼Œä¸å¤ªæœƒè¢«ç¿»è­¯
                marker = f"999{i:03d}999"  # ä¾‹å¦‚: 999000999, 999001999
                word_map[marker] = word
                
                # ä½¿ç”¨è©é‚Šç•ŒåŒ¹é…ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
                import re
                # å°æ–¼åŒ…å«ç©ºæ ¼çš„çŸ­èªï¼Œç›´æ¥æ›¿æ›
                if ' ' in word:
                    protected_text = protected_text.replace(word, marker)
                else:
                    # å°æ–¼å–®è©ï¼Œä½¿ç”¨è©é‚Šç•ŒåŒ¹é…
                    pattern = r'\b' + re.escape(word) + r'\b'
                    protected_text = re.sub(pattern, marker, protected_text, flags=re.IGNORECASE)
                
                logger.info(f"    ğŸ›¡ï¸ Protected: '{word}' â†’ {marker}")
            else:
                logger.info(f"    âŒ Word '{word}' not found in text")
        
        logger.info(f"ğŸ” DEBUG: Protected text: '{protected_text[:100]}...'")
        
        # æ­¥é©Ÿ2: ç¿»è­¯ä¿è­·å¾Œçš„æ–‡å­—
        translated_text = self._translate_text(protected_text, source_lang, target_lang, translate_client)
        logger.info(f"ğŸ” DEBUG: Translated text: '{translated_text[:100]}...'")
        
        # æ­¥é©Ÿ3: æ¢å¾©åŸå§‹è©å½™
        for marker, original_word in word_map.items():
            if marker in translated_text:
                translated_text = translated_text.replace(marker, original_word)
                logger.info(f"    ğŸ”„ Restored: {marker} â†’ '{original_word}'")
            else:
                logger.warning(f"    âš ï¸ Marker {marker} not found in translation!")
                # æª¢æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†æ¨™è¨˜æ®˜ç•™
                partial_markers = [m for m in translated_text.split() if '999' in m and m.isdigit()]
                if partial_markers:
                    logger.warning(f"    âš ï¸ Found partial markers: {partial_markers}")
                    for partial in partial_markers:
                        translated_text = translated_text.replace(partial, original_word)
                        logger.info(f"    ğŸ”„ Fixed partial marker: {partial} â†’ '{original_word}'")
        
        logger.info(f"ğŸ” DEBUG: Final text: '{translated_text[:100]}...'")
        return translated_text
    
    def _translate_text(self, text: str, source_lang: str, target_lang: str, translate_client) -> str:
        """ç¿»è­¯æ–‡å­—"""
        try:
            # æª¢æŸ¥æ˜¯å¦åŒ…å«æ›è¡Œï¼Œå¦‚æœæœ‰å‰‡åˆ†æ®µè™•ç†
            if '\n' in text:
                lines = text.split('\n')
                translated_lines = []
                
                for line in lines:
                    line = line.strip()
                    if not line:  # ç©ºè¡Œ
                        translated_lines.append('')
                        continue
                    
                    # ç¿»è­¯æ¯ä¸€è¡Œ
                    response = translate_client.translate_text(
                        Text=line,
                        SourceLanguageCode=source_lang,
                        TargetLanguageCode=target_lang
                    )
                    translated_line = response['TranslatedText']
                    
                    # å¾Œè™•ç†ï¼šæ”¹å–„ç¿»è­¯è³ªé‡
                    improved_line = self._improve_translation_quality(translated_line, line)
                    translated_lines.append(improved_line)
                
                # é‡æ–°çµ„åˆï¼Œä¿æŒæ›è¡Œ
                return '\n'.join(translated_lines)
            
            else:
                # å–®è¡Œæ–‡æœ¬ç›´æ¥ç¿»è­¯
                response = translate_client.translate_text(
                    Text=text,
                    SourceLanguageCode=source_lang,
                    TargetLanguageCode=target_lang
                )
                translated_text = response['TranslatedText']
                
                # å¾Œè™•ç†ï¼šæ”¹å–„ç¿»è­¯è³ªé‡
                improved_text = self._improve_translation_quality(translated_text, text)
                return improved_text
            
        except Exception as e:
            logger.error(f"âŒ Translation API failed: {e}")
            return text  # è¿”å›åŸæ–‡
    
    def _improve_translation_quality(self, translation: str, original_text: str) -> str:
        """é€šç”¨ç¿»è­¯è³ªé‡æ”¹å–„ - åªåšåŸºæœ¬çš„æ ¼å¼æ¸…ç†"""
        import re
        improved = translation
        
        # 1. åŸºæœ¬æ ¼å¼æ¸…ç†ï¼ˆé€šç”¨ï¼‰
        # æ¸…ç†å¤šé¤˜ç©ºæ ¼
        improved = re.sub(r'\s+', ' ', improved).strip()
        
        # æ¸…ç†æ˜é¡¯çš„æ¨™é»ç¬¦è™ŸéŒ¯èª¤ï¼ˆé€šç”¨ï¼‰
        improved = re.sub(r'ã€‚ã€', 'ã€‚', improved)
        improved = re.sub(r'ï¼Œã€', 'ï¼Œ', improved)
        improved = re.sub(r'ã€([ã€‚ï¼Œï¼ï¼Ÿ])', r'\1', improved)
        
        # 2. è™•ç†é …ç›®ç¬¦è™Ÿæ ¼å¼ï¼ˆé€šç”¨ï¼‰
        if original_text.strip().startswith(('â€¢', '-', 'â–º')):
            if not improved.strip().startswith(('â€¢', '-', 'â–º')):
                if original_text.strip().startswith('â€¢'):
                    improved = 'â€¢ ' + improved.strip()
                elif original_text.strip().startswith('-'):
                    improved = '- ' + improved.strip()
                elif original_text.strip().startswith('â–º'):
                    improved = 'â–º ' + improved.strip()
        
        return improved
        """é¸æ“‡æœ€ä½³ç¿»è­¯çµæœï¼ŒåŸºæ–¼é€šç”¨çš„å¹»è¦ºæª¢æ¸¬"""
        if len(translations) == 1:
            # å³ä½¿åªæœ‰ä¸€å€‹ç¿»è­¯ï¼Œä¹Ÿè¦æª¢æŸ¥å’Œä¿®æ­£å¹»è¦º
            return self._fix_common_hallucinations(translations[0], original_text)
        
        # é€šç”¨å¹»è¦ºæª¢æ¸¬è¦å‰‡
        def has_hallucination_signs(translation: str, original: str) -> int:
            """è¿”å›å¹»è¦ºæŒ‡æ•¸ï¼Œè¶Šé«˜è¶Šå¯èƒ½æ˜¯å¹»è¦º"""
            score = 0
            
            # 1. æª¢æŸ¥æ˜¯å¦åŒ…å«æ˜é¡¯ä¸ç›¸é—œçš„æ•¸å­—å¹´ä»½
            import re
            year_pattern = r'ä¸€ä¹\d+å¹´|äºŒã€‡\d+å¹´|\d{4}å¹´'
            if re.search(year_pattern, translation) and not re.search(r'\d{4}', original):
                score += 20  # æé«˜åˆ†æ•¸ï¼Œé€™æ˜¯åš´é‡çš„å¹»è¦º
                logger.warning(f"ğŸš¨ Detected suspicious year in translation: {translation}")
            
            # 2. æª¢æŸ¥ç‰¹å®šçš„å¹»è¦ºæ¨¡å¼
            hallucination_patterns = [
                r'ä¸€ä¹+å¹´',  # é€£çºŒçš„ä¸€ä¹å¹´
                r'ä¹+å¹´',    # é€£çºŒçš„ä¹å¹´
                r'åœ‹éš›.*ç½²', # åœ‹éš›XXç½²
                r'å·¥å•†.*å±€', # å·¥å•†XXå±€
            ]
            for pattern in hallucination_patterns:
                if re.search(pattern, translation):
                    score += 15
                    logger.warning(f"ğŸš¨ Detected hallucination pattern: {pattern}")
            
            # 3. æª¢æŸ¥é•·åº¦æ¯”ä¾‹ç•°å¸¸
            original_len = len(original.split())
            translation_len = len(translation)
            if original_len > 0:
                ratio = translation_len / original_len
                if ratio > 4 or ratio < 0.2:  # ç¿»è­¯çµæœé•·åº¦åš´é‡ç•°å¸¸
                    score += 10
                    logger.warning(f"ğŸš¨ Suspicious length ratio: {ratio}")
            
            # 4. æª¢æŸ¥è‹±æ–‡ç¸®å¯«æ˜¯å¦è¢«éŒ¯èª¤ç¿»è­¯
            english_abbrevs = re.findall(r'\b[A-Z]{2,}\b', original)
            for abbrev in english_abbrevs:
                if abbrev not in translation and len(abbrev) <= 5:
                    score += 5
                    logger.warning(f"ğŸš¨ Missing abbreviation: {abbrev}")
            
            return score
        
        # è©•ä¼°æ¯å€‹ç¿»è­¯
        best_translation = translations[0]
        lowest_score = float('inf')
        
        for i, translation in enumerate(translations):
            score = has_hallucination_signs(translation, original_text)
            logger.info(f"Translation {i+1} hallucination score: {score}")
            logger.info(f"Translation {i+1}: {translation[:100]}...")
            
            if score < lowest_score:
                lowest_score = score
                best_translation = translation
        
        # ä¿®æ­£é¸ä¸­çš„ç¿»è­¯
        corrected_translation = self._fix_common_hallucinations(best_translation, original_text)
        
        if lowest_score > 0:
            logger.warning(f"âš ï¸ Applied corrections to translation with hallucination score: {lowest_score}")
        else:
            logger.info("âœ… Selected translation appears clean")
        
        return corrected_translation
    
    def _fix_common_hallucinations(self, translation: str, original_text: str) -> str:
        """é€šç”¨å¹»è¦ºä¿®æ­£æ©Ÿåˆ¶ - åŸºæ–¼æ¨¡å¼è€Œéç‰¹å®šè©å½™"""
        import re
        corrected = translation
        
        # 1. é€šç”¨å¹´ä»½å¹»è¦ºæª¢æ¸¬
        # å¦‚æœåŸæ–‡æ²’æœ‰å¹´ä»½ï¼Œä½†ç¿»è­¯ä¸­å‡ºç¾äº†æ˜é¡¯çš„å¹»è¦ºå¹´ä»½æ¨¡å¼
        has_year_in_original = bool(re.search(r'\b\d{4}\b', original_text))
        
        if not has_year_in_original:
            # ç§»é™¤æ˜é¡¯çš„å¹»è¦ºå¹´ä»½æ¨¡å¼ï¼ˆé€£çºŒé‡è¤‡çš„æ•¸å­—å¹´ä»½ï¼‰
            hallucination_patterns = [
                r'ä¸€ä¹{3,}\d*å¹´?',  # ä¸€ä¹ä¹ä¹ä¹å¹´ç­‰ï¼ˆ3å€‹ä»¥ä¸Šé‡è¤‡ï¼‰
                r'äºŒã€‡{3,}\d*å¹´?',  # äºŒã€‡ã€‡ã€‡ã€‡å¹´ç­‰
                r'ä¹{3,}\d*å¹´?',   # ä¹ä¹ä¹ä¹å¹´ç­‰
                r'é›¶{2,}\d*å¹´?',   # é›¶é›¶é›¶å¹´ç­‰
            ]
            
            for pattern in hallucination_patterns:
                matches = re.findall(pattern, corrected)
                if matches:
                    corrected = re.sub(pattern, '', corrected)
                    logger.info(f"ğŸ”§ Removed hallucination year pattern: {matches}")
        
        # 2. æª¢æ¸¬å’Œä¿®æ­£ç•°å¸¸é•·åº¦çš„ç¿»è­¯
        original_length = len(original_text.split())
        translation_length = len(corrected)
        
        if original_length > 0:
            ratio = translation_length / original_length
            if ratio > 10:  # ç¿»è­¯çµæœç•°å¸¸é•·ï¼Œå¯èƒ½åŒ…å«å¹»è¦º
                logger.warning(f"âš ï¸ Translation unusually long (ratio: {ratio:.1f})")
                # å¯ä»¥è€ƒæ…®æˆªæ–·æˆ–é‡æ–°ç¿»è­¯
        
        # 3. ä¿è­·åŸæ–‡ä¸­çš„é‡è¦è¡“èª
        # æå–åŸæ–‡ä¸­çš„è‹±æ–‡ç¸®å¯«å’Œå°ˆæœ‰åè©
        important_terms = []
        
        # è‹±æ–‡ç¸®å¯«ï¼ˆ2-6å€‹å¤§å¯«å­—æ¯ï¼‰
        abbreviations = re.findall(r'\b[A-Z]{2,6}\b', original_text)
        important_terms.extend(abbreviations)
        
        # å°ˆæœ‰åè©ï¼ˆé¦–å­—æ¯å¤§å¯«çš„è©ï¼‰
        proper_nouns = re.findall(r'\b[A-Z][a-z]{2,}\b', original_text)
        important_terms.extend(proper_nouns)
        
        # å»é‡
        important_terms = list(set(important_terms))
        
        # æª¢æŸ¥é‡è¦è¡“èªæ˜¯å¦åœ¨ç¿»è­¯ä¸­ä¿ç•™
        missing_terms = []
        for term in important_terms:
            if len(term) <= 20 and term not in corrected:
                missing_terms.append(term)
        
        if missing_terms:
            logger.warning(f"âš ï¸ Important terms missing in translation: {missing_terms}")
            # æ³¨æ„ï¼šé€™è£¡åªè¨˜éŒ„ï¼Œä¸è‡ªå‹•ä¿®æ­£ï¼Œå› ç‚ºå¯èƒ½æœ‰åˆç†çš„ç¿»è­¯
        
        # 4. æ¸…ç†æ ¼å¼å•é¡Œ
        # ç§»é™¤å¤šé¤˜ç©ºæ ¼
        corrected = re.sub(r'\s+', ' ', corrected).strip()
        
        # ç§»é™¤é–‹é ­çš„å­¤ç«‹æ¨™é»ç¬¦è™Ÿ
        corrected = re.sub(r'^[-â€“â€”â€¢]\s*', '', corrected)
        
        # ç§»é™¤æ˜é¡¯çš„æ ¼å¼éŒ¯èª¤ï¼ˆå¦‚é€£çºŒçš„æ¨™é»ç¬¦è™Ÿï¼‰
        corrected = re.sub(r'[ã€‚ï¼Œ]{2,}', 'ï¼Œ', corrected)
        
        return corrected

    def _create_translation_text_file(self, original_pages: List[str], translated_pages: List[str], output_path: str) -> bool:
        """å‰µå»ºç´”æ–‡å­—ç¿»è­¯æ–‡ä»¶"""
        try:
            # æ”¹è®Šè¼¸å‡ºæ–‡ä»¶ç‚º.txtæ ¼å¼
            txt_output_path = output_path.replace('.pdf', '_translation.txt')
            
            logger.info(f"ğŸ“ Creating text file at: {txt_output_path}")
            
            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            output_dir = os.path.dirname(txt_output_path)
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
            
            with open(txt_output_path, 'w', encoding='utf-8') as f:
                f.write("AWS PDF Translation Report\n")
                f.write("=" * 50 + "\n\n")
                
                for i, (original, translated) in enumerate(zip(original_pages, translated_pages)):
                    f.write(f"ğŸ“„ Page {i+1}\n")
                    f.write("-" * 30 + "\n\n")
                    
                    f.write("ğŸ”¤ Original Text:\n")
                    f.write(original + "\n\n")
                    
                    f.write("ğŸŒ Chinese Translation:\n")
                    f.write(translated + "\n\n")
                    
                    f.write("=" * 50 + "\n\n")
            
            # é©—è­‰æ–‡ä»¶å‰µå»º
            if os.path.exists(txt_output_path) and os.path.getsize(txt_output_path) > 0:
                logger.info(f"âœ… Translation text file created: {txt_output_path}")
                return True
            else:
                logger.error(f"âŒ Text file not created properly")
                return False
            
        except Exception as e:
            logger.error(f"âŒ Failed to create text file: {e}")
            return False
    
    def _generate_status_report(self, pages_count: int, output_path: str, 
                               original_pages: List[str] = None, translated_pages: List[str] = None) -> str:
        """ç”Ÿæˆè©³ç´°ç‹€æ…‹å ±å‘Š"""
        report = f"""ğŸ“Š AWS PDF Translation Report
========================================
âœ… Status: Completed Successfully
ğŸ“„ Pages processed: {pages_count}
ğŸ“ Output file: {os.path.basename(output_path)}
ğŸŒ Service: Amazon Translate + Bedrock AI
========================================

ğŸ“ Translation Preview:
"""
        
        # æ·»åŠ ç¿»è­¯é è¦½
        if original_pages and translated_pages:
            for i, (original, translated) in enumerate(zip(original_pages[:3], translated_pages[:3])):
                report += f"\nğŸ“„ Page {i+1}:\n"
                report += f"Original: {original[:150]}{'...' if len(original) > 150 else ''}\n"
                report += f"Translation: {translated[:150]}{'...' if len(translated) > 150 else ''}\n"
                report += "-" * 40 + "\n"
        
        report += f"\nâœ… Complete translation saved to: {output_path}"
        return report
    
    def _create_success_image(self) -> torch.Tensor:
        """å‰µå»ºæˆåŠŸç‹€æ…‹åœ–åƒ"""
        # å‰µå»ºç°¡å–®çš„æˆåŠŸç‹€æ…‹åœ–åƒ
        img = Image.new('RGB', (512, 256), color='lightgreen')
        draw = ImageDraw.Draw(img)
        
        try:
            # å˜—è©¦ä½¿ç”¨ç³»çµ±å­—é«”
            font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 24)
        except:
            font = ImageFont.load_default()
        
        # ç¹ªè£½æˆåŠŸä¿¡æ¯
        draw.text((50, 100), "âœ… PDF Translation", fill='darkgreen', font=font)
        draw.text((50, 140), "Completed Successfully!", fill='darkgreen', font=font)
        
        # è½‰æ›ç‚ºtensor
        img_array = np.array(img).astype(np.float32) / 255.0
        img_tensor = torch.from_numpy(img_array)[None,]
        
        return img_tensor
    
    def _create_error_result(self, error_message: str) -> Tuple[torch.Tensor, str]:
        """å‰µå»ºéŒ¯èª¤çµæœ"""
        # å‰µå»ºéŒ¯èª¤ç‹€æ…‹åœ–åƒ
        img = Image.new('RGB', (512, 256), color='lightcoral')
        draw = ImageDraw.Draw(img)
        
        try:
            font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 20)
        except:
            font = ImageFont.load_default()
        
        draw.text((50, 100), "âŒ Translation Failed", fill='darkred', font=font)
        draw.text((50, 140), error_message[:30], fill='darkred', font=font)
        
        # è½‰æ›ç‚ºtensor
        img_array = np.array(img).astype(np.float32) / 255.0
        img_tensor = torch.from_numpy(img_array)[None,]
        
        error_report = f"""âŒ AWS PDF Translation Error
========================================
Error: {error_message}
========================================"""
        
        return (img_tensor, error_report)

# ç¯€é»æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "AWSPDFTranslator": AWSPDFTranslator
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AWSPDFTranslator": "AWS PDF Translator"
}
